)
.groups = 'drop'
# Create the dataframe
# ***********Final dataset for state map and state level time series trend
extended_data <- melted_data %>%
left_join(subdata1, by = c("Type", "Date")) %>%
left_join(subdata2, by = c("Date","Type", "state"))%>%
left_join(subdata3, by = c("Date","Type", "state", "value")) %>%
mutate(Percent_byStateParty=TotalTrial_byStatebyParty/TotalTrial_byState) %>%
mutate(Date=as.Date(Date)) %>%
mutate(year = as.numeric(format(as.Date(Date), "%Y")))%>%
mutate(abb=state)%>%
mutate(Predicted_party=ifelse(Percent_byState>=0.5, "Republican", "Democratic")) %>%
select(-StateFull.y) %>%
rename(
StateFull=StateFull.x)
extended_data2<-extended_data%>%
select(Date,Type,party,StateFull,state,Percent_byState_Demo,Percent_byState_chr_Demo,Percent_byStateParty,Predicted_party)%>%
distinct(Date,Type,party,StateFull,state,Percent_byState_Demo,Percent_byState_chr_Demo,Percent_byStateParty,Predicted_party)
# Create the dc_data with DC information for each Date and Type
dates <-unique(extended_data2$Date)
dc_data <- expand.grid(
state = "DC",
StateFull = "District of Columbia",
party = "Democratic",
Predicted_party = "Democratic",
Percent_byState_Demo = 1,
Percent_byState_chr_Demo = "100.00%",
Percent_byStateParty = 1,
Type = c("Direct", "MSNBC", "BBC", "Fox"),  # The four types
Date = dates
)
# Ensure the column order is the same as in extended_data2
# Combine the original data with the dc_data and ensure it is sorted by Date, Type, and state
extended_data2 <- rbind(extended_data2, dc_data) %>%
arrange(Date, Type, state)
# method #1 calculate the average winning probability in each state and assign the electoral votes
subdata2_1 <-subdata2%>%
left_join(electoral_votes, by="state")%>%
select(Date,Type,state,Percent_byState_Demo,Electoral_Votes)%>%
#filter(Date=="2024-08-13")%>%
mutate(proj_party=ifelse(Percent_byState_Demo>=0.5, "Democratic", "Republican"))
subdata2_2 <-subdata2_1%>%
group_by(Date, proj_party,Type) %>%
summarize(
Total_votes = sum(Electoral_Votes)
)
# number should be 535 for now
trials<-subdata2_1 %>%
group_by(Date,Type)%>%
summarize(
Votes = sum(Electoral_Votes)
)
votes_trials_percent<-subdata2_2 %>%
left_join(trials,by=c("Type","Date")) %>%
mutate(votes_percent=Total_votes/Votes) %>%
select(Date, Type,proj_party, votes_percent) %>%
pivot_wider(names_from = Type, values_from = votes_percent, names_prefix = "Votes_percent_")
votes_trials<-subdata2_2 %>%
select(Date, Type,proj_party, Total_votes) %>%  # Select relevant columns
pivot_wider(names_from = Type, values_from = Total_votes, names_prefix = "Votes_")%>%
arrange(Date)
subdata2_reshape <-votes_trials%>%
left_join(votes_trials_percent, by=c("Date","proj_party")) %>%
filter(proj_party=="Democratic")%>%
arrange(Date) %>%
filter(proj_party=="Democratic")
.groups = 'drop'
#*******************************************************************
subdata2_reshape <- subdata2_reshape %>%
ungroup()
#*******************************************************************
##method #2 assign the votes first and then calculate the average
subdata3_1<-melted_data %>%
group_by(Type, Trial, Date,party) %>%
summarise(
Number_Repub_Win = n(),
votes_party =sum(Electoral_Votes),
.groups = 'drop'
)
subdata3_1_2<-melted_data %>%
group_by(Type, Date, party) %>%
summarise(
Number_Repub_Win = n(),
votes_party =sum(Electoral_Votes),
.groups = 'drop'
) %>%
arrange(Date)
#calculate total number of votes
subdata3_2_2<-melted_data %>%
group_by(Type, Date) %>%
summarise(
Votes =sum(Electoral_Votes),
.groups = 'drop'
)
subdata3_2_reshape<-subdata3_1_2%>%
left_join(subdata3_2_2,by=c("Type","Date")) %>%
mutate(Votes_perent=votes_party/Votes) %>%
select(Date, Type,party, Votes_perent) %>%  # Select relevant columns
pivot_wider(names_from = Type, values_from = Votes_perent, names_prefix = "Votes_")%>%
filter(party=="Democratic")%>%
arrange(Date)
.groups = 'drop'
# calculate total trials for each type each date for both parties, should be 100 per day for each type but we lose trials
trial_counts<-subdata3_1 %>%
group_by(Type,Date) %>%
mutate(trial_counts= n_distinct(Trial))%>%
select(Type,Date,party,trial_counts)%>%
distinct() # drop duplicates
.groups = 'drop'
trial_votes<-melted_data %>%
group_by(Type, Date,party) %>%
summarise(
votes_party =sum(Electoral_Votes),
.groups = 'drop'
) %>%
left_join(trial_counts, by=c("party","Type","Date" ))%>%
mutate(average_votes=votes_party/trial_counts)
#total should be 535 for now and 538 later
total_votes_count <- trial_votes %>%
select(Date, Type, average_votes) %>%
group_by(Type, Date)%>%
summarise(
votes_total=sum(average_votes)
)
# merge two datasets before reshape
trial_votes <-trial_votes%>%
left_join(total_votes_count, by=c("Type","Date" )) %>%
mutate(average_votes = round(average_votes, digits = 0)) %>%  # Round average_votes
mutate(average_votes_percent=average_votes/votes_total)
#*************** Dataset with predicted votes
#* For table
average_votes<- trial_votes %>%
select(Date, Type, party, average_votes)%>%
rename(
Voice = Type,  # Renaming 'Result' to 'value'
Party= party,
AverageVotes=average_votes)
#For graph
average_votes_reshape <- trial_votes %>%
select(Date, Type, party, average_votes) %>%  # Select relevant columns
pivot_wider(names_from = Type, values_from = average_votes, names_prefix = "Votes_")
# For table
average_votes_percent<- trial_votes %>%
select(Date, Type, party, average_votes_percent) %>%
mutate(average_votes_percent = round(average_votes_percent, digits = 2))%>%
rename(
Voice = Type,  # Renaming 'Result' to 'value'
Party= party,
WinLikelihood=average_votes_percent)
# For graph
average_votes_percent_reshape<- trial_votes %>%
select(Date, Type, party, average_votes_percent) %>%  # Select relevant columns
pivot_wider(names_from = Type, values_from = average_votes_percent, names_prefix = "Votes_Percent_")
# Final Dataset for national level winner for average votes and average votes percent
#*******************************************************************
trial_votes_reshape <-average_votes_reshape%>%
left_join(average_votes_percent_reshape, by=c("Date","party"))%>%
filter(party=="Democratic")%>%
arrange(Date)
#*******************************************************************
#-------------import expert data
expert <-read_csv("Expert_Opinions.csv",show_col_types = FALSE)
expert_data<-expert%>%
rename(
Date = date,  # Renaming 'Result' to 'value'
)%>%
mutate(Date = as.Date(Date, format = "%m/%d/%y")) %>%
mutate(Silver = round(Silver/100,digit=2)) %>%
mutate(Times = round(Times/100,digit=2)) %>%
mutate(party="Democratic")
expert_all<-average_votes_percent_reshape%>%
filter(party=="Democratic") %>%
left_join(expert_data,by=c("Date","party"))
#------------Data  process done
#write_csv(extended_data,"/Users/sunmingrun/Desktop/AI Project/panel_election_results_help.csv")
#------color
pal <- pnw_palette(name = "Bay", n = 16, type = "continuous")
color_for_1 <- pal[16]  # Close to red
color_for_0 <- pal[1]  # Close to blue
#pal2<- pnw_palette("Moth",12)
#color_for_low1 <- pal2[6]
#pal4<-pnw_palette(name = "Bay", n = 16, type = "continuous")
#color_for_low0 <- pal4[5]
pal2<-pnw_palette(name = "Bay", n = 16, type = "continuous")
color_for_low1 <- pal2[13]
pal3<- pnw_palette("Shuksan2",5)
color_for_low0 <- pal3[2]
pal5 <- pnw_palette("Winter",100)
color_for_05 <- pal5[97]
custom_colorscale <- list(
list(0, "#DD4124"),       # Red at the low end (0) (Democrats)
list(0.25, color_for_low1),    # Lighter red between 0 and 0.5
list(0.5, color_for_05),     # Very light red/pink at the middle (0.5) "#ffe6e6"
list(0.75, color_for_low0),    # Lighter blue between 0.5 and 1
list(1, "#00496F")        # Blue at the high end (1) (Republicans)
)
#data
#function 1 state unique label
state_group <- subdata2 %>%
select(StateFull) %>%
arrange(StateFull) %>%
distinct()
voice_group <- extended_data %>%
select(Type) %>%
arrange(Type) %>%
distinct()
party_group <- extended_data %>%
select(party) %>%
arrange(party) %>%
distinct()
filtered_data <- extended_data %>%
filter(Date == "2024-08-15")
distinct <- filtered_data %>%
distinct(Type,Date, No_Republican,No_Democratic,Republican,Democratic,TotalTrial_byState)
steps<-read_csv2("help.csv",show_col_types = FALSE)
intro <- read_csv2("intro.csv",show_col_types = FALSE)
expert_all
expert_all_rename<-expert_all%>%
rename(
Anonymous=Votes_Percent_Anonymous,
BBC=Votes_Percent_BBC,
FOX=Votes_Percent_Fox,
"Nate Sliver"=Silver,
"New York Times"=Times
)
expert_all_rename
expert_all_rename<-expert_all%>%
rename(
Anonymous=Votes_Percent_Anonymous,
BBC=Votes_Percent_BBC,
FOX=Votes_Percent_Fox,
MSNBC=Votes_Percent_MSNBC
"Nate Sliver"=Silver,
runApp()
expert_all_rename<-expert_all%>%
rename(
Anonymous=Votes_Percent_Anonymous,
BBC=Votes_Percent_BBC,
Fox=Votes_Percent_Fox,
MSNBC=Votes_Percent_MSNBC,
"Nate Sliver"=Silver,
"New York Times"=Times
) %>%
mutate(Anonymous=round(Anonymous,digits=2)) %>%
mutate(BBC=round(BBC,digits=2)) %>%
mutate(Fox=round(Fox,digits=2)) %>%
mutate(MSNBC=round(MSNBC,digits=2))
expert_all_rename
runApp()
runApp()
runApp()
extended_data2
extended_data2_check<-extended_data2%>%
filter(Type=="Fox")
extended_data2_check
extended_data2_check<-data%>%
filter(Type=="Fox")
extended_data2_check<-melted_data%>%
filter(Type=="Fox")
extended_data2_check
extended_data2_check<-melted_data%>%
filter(Type=="Fox")%>%
filter(state="GA")
View(extended_data2_check)
extended_data2_check<-melted_data%>%
filter(Type=="Fox")%>%
filter(state=="GA")
View(extended_data2_check)
runApp()
extended_data2_check<-melted_data%>%
filter(Type=="Fox")%>%
filter(state=="NC")%>%
filter(Date=="2024-8-20")
View(extended_data2_check)
runApp()
install.packages("cronR")
install.packages("httr")
install.packages("cronR")
download_panel_data <- function() {
# URL of your CSV file in the GitHub repository
url <- "https://raw.githubusercontent.com/scunning1975/KamalaGPT/main/news_data/election_news/panelel-ection_results_state.csv"
# Destination path to save the CSV
destfile <- "/Users/sunmingrun/Documents/GitHub/ElectionGPT/ShinyApp"
# Download the file
GET(url, write_disk(destfile, overwrite = TRUE))
}
# Run the function to download the latest CSV
download_panel_data()
# download automate process
library(httr)
download_panel_data <- function() {
# URL of your CSV file in the GitHub repository
url <- "https://raw.githubusercontent.com/scunning1975/KamalaGPT/main/news_data/election_news/panelel-ection_results_state.csv"
# Destination path to save the CSV
destfile <- "/Users/sunmingrun/Documents/GitHub/ElectionGPT/ShinyApp"
# Download the file
GET(url, write_disk(destfile, overwrite = TRUE))
}
# Run the function to download the latest CSV
download_panel_data()
library(httr)
library(rsconnect)
destfile <- "/Users/sunmingrun/Desktop/untitled folder"
# download automate process
library(httr)
download_panel_data <- function() {
# URL of your CSV file in the GitHub repository
url <- "https://raw.githubusercontent.com/scunning1975/KamalaGPT/main/news_data/election_news/panelel-ection_results_state.csv"
# Destination path to save the CSV
destfile <- "/Users/sunmingrun/Desktop/untitled folder"
# Download the file
GET(url, write_disk(destfile, overwrite = TRUE))
}
# Run the function to download the latest CSV
download_panel_data()
download_panel_data <- function() {
# URL to raw CSV file on GitHub
url <- "https://raw.githubusercontent.com/scunning1975/KamalaGPT/main/news_data/election_news/panelel-ection_results_state.csv"
# Make sure to provide a valid path where the file will be saved
destfile <- "/Users/sunmingrun/Desktop/untitled_folder/panel_data.csv" # Corrected folder path
# Check if the folder exists; if not, create it
if (!dir.exists("/Users/sunmingrun/Desktop/untitled_folder")) {
dir.create("/Users/sunmingrun/Desktop/untitled_folder")
}
# Download the file
GET(url, write_disk(destfile, overwrite = TRUE))
}
download_panel_data <- function() {
# URL to raw CSV file on GitHub
url <- "https://raw.githubusercontent.com/scunning1975/KamalaGPT/main/news_data/election_news/panelel-ection_results_state.csv"
# Make sure to provide a valid path where the file will be saved
destfile <- "/Users/sunmingrun/Desktop/untitled_folder/panel_data.csv" # Corrected folder path
# Download the file
GET(url, write_disk(destfile, overwrite = TRUE))
}
# Run the function to download the latest CSV
download_panel_data()
# download automate process
library(httr)
# download automate process
library(httr)
download_panel_data <- function() {
# URL of your CSV file in the GitHub repository
url <- "https://raw.githubusercontent.com/scunning1975/KamalaGPT/main/news_data/election_news/panelel-ection_results_state.csv"
# Destination path to save the CSV
destfile <- "/Users/sunmingrun/Documents/GitHub/ElectionGPT/ShinyApp"
# Download the file
GET(url, write_disk(destfile, overwrite = TRUE))
}
# Run the function to download the latest CSV
download_panel_data()
library(httr)
library(httr)
download_panel_data <- function() {
# URL of your CSV file in the GitHub repository
url <- "https://raw.githubusercontent.com/scunning1975/KamalaGPT/main/news_data/election_news/panelel-ection_results_state.csv"
# Destination path to save the CSV, including the file name
destfile <- "/Users/sunmingrun/Documents/GitHub/ElectionGPT/ShinyApp/panel_data.csv"
# Download the file and overwrite if it exists
GET(url, write_disk(destfile, overwrite = TRUE))
}
# Run the function to download the latest CSV
download_panel_data()
#Step 2: Use Git to add, commit, and push changes to GitHub
push_to_github <- function() {
# Navigate to your local GitHub repository
setwd("/Users/sunmingrun/Documents/GitHub/ElectionGPT")
# Add the updated files to the staging area
system("git add .")  # Add all changed files, or you can specify the specific file: system('git add path_to_file')
# Commit the changes with a message
system("git commit -m 'Automated update of panel_data.csv and Shiny app'")
# Push the changes to GitHub
system("git push origin main")
message("Changes pushed to GitHub successfully.")
}
# Step 3: Automate the process by running both functions
automate_process <- function() {
download_panel_data()
push_to_github()
}
# Run the entire process
automate_process()
library(httr)
#Step 1
download_panel_data <- function() {
url <- "https://raw.githubusercontent.com/scunning1975/KamalaGPT/main/news_data/election_news/panelel-ection_results_state.csv"
destfile <- "/Users/sunmingrun/Documents/GitHub/ElectionGPT/ShinyApp/panel_data.csv"
GET(url, write_disk(destfile, overwrite = TRUE))
}
#Step 2: Use Git to add, commit, and push changes to GitHub
push_to_github <- function() {
# Navigate to your local GitHub repository
setwd("/Users/sunmingrun/Documents/GitHub/ElectionGPT/ShinyApp")
# Add the updated files to the staging area
system("git add .")  # Add all changed files, or you can specify the specific file: system('git add path_to_file')
# Commit the changes with a message
system("git commit -m 'Automated update of panel_data.csv'")
# Push the changes to GitHub
system("git push origin main")
message("Changes pushed to GitHub successfully.")
}
# Step 3: Automate the process by running both functions
automate_process <- function() {
download_panel_data()
push_to_github()
}
# Run the entire process
automate_process()
#Step 2: Use Git to add, commit, and push changes to GitHub
push_to_github <- function() {
# Navigate to your local GitHub repository
setwd("/Users/sunmingrun/Documents/GitHub/ElectionGPT/ShinyApp")
# Add the updated files to the staging area
system("git add .")  # Add all changed files, or you can specify the specific file: system('git add path_to_file')
# Commit the changes with a message
system("git commit -m 'Automated update of panel_data.csv'")
# Push the changes to GitHub
system("git push origin main")
message("Changes pushed to GitHub successfully.")
}
# Step 3: Automate the process by running both functions
automate_process <- function() {
download_panel_data()
push_to_github()
}
# Run the entire process
automate_process()
library(httr)
#Step 1
download_panel_data <- function() {
url <- "https://raw.githubusercontent.com/scunning1975/KamalaGPT/main/news_data/election_news/panelel-ection_results_state.csv"
destfile <- "/Users/sunmingrun/Documents/GitHub/ElectionGPT/ShinyApp/panel_data.csv"
GET(url, write_disk(destfile, overwrite = TRUE))
}
#Step 1
download_panel_data <- function() {
url <- "https://raw.githubusercontent.com/scunning1975/KamalaGPT/main/news_data/election_news/panelel-ection_results_state.csv"
destfile <- "/Users/sunmingrun/Documents/GitHub/ElectionGPT/ShinyApp/panel_data.csv"
GET(url, write_disk(destfile, overwrite = TRUE))
}
#Step 2: Use Git to add, commit, and push changes to GitHub
push_to_github <- function() {
# Navigate to your local GitHub repository
setwd("/Users/sunmingrun/Documents/GitHub/ElectionGPT/ShinyApp")
# Add the updated files to the staging area
system("git add .")  # Add all changed files, or you can specify the specific file: system('git add path_to_file')
# Commit the changes with a message
system("git commit -m 'Automated update of panel_data.csv'")
# Push the changes to GitHub
system("git push origin main")
message("Changes pushed to GitHub successfully.")
}
# Step 3: Automate the process by running both functions
automate_process <- function() {
download_panel_data()
push_to_github()
}
# Run the entire process
automate_process()
shiny::runApp()
setwd("/Users/sunmingrun/Documents/GitHub/ElectionGPT/NLP")
data <- read_csv(/Users/sunmingrun/Documents/GitHub/ElectionGPT/NLP/combined_news_data.csv,show_col_types = FALSE)
data <- read_csv("combined_news_data.csv",show_col_types = FALSE)
data
data_cleaned <-data %>%
select(date,sentiment,relevance,source,source.title)
data_cleaned <-data %>%
select(date,sentiment,relevance,source.title)
data_cleaned
data_cleaned <-data %>%
select(date,sentiment,relevance,source.uri,source.title)
View(data_cleaned)
data_cleaned <-data %>%
select(date,sentiment,relevance,source.uri,source.title) %>%
arrange(date)
View(data_cleaned)
write_csv("news_data.csv")
write_csv
write_csv(data_cleaned,"news_data.csv")
write_csv(newsdata_cleaned,"news_data.csv")
data <- read_csv("combined_news_data.csv",show_col_types = FALSE)
data_cleaned <-data %>%
select(date,sentiment,relevance,source.uri,source.title) %>%
arrange(date)
#View(data_cleaned)
write_csv(newsdata_cleaned,"news_data.csv")
data <- read_csv("combined_news_data.csv",show_col_types = FALSE)
data_cleaned <-data %>%
select(date,sentiment,relevance,source.uri,source.title) %>%
arrange(date)
#View(data_cleaned)
write_csv(newsdata_cleaned,"news_data_cleaned.csv")
data <- read_csv("combined_news_data.csv",show_col_types = FALSE)
data_cleaned <-data %>%
select(date,sentiment,relevance,source.uri,source.title) %>%
arrange(date)
write_csv(newsdata_cleaned,"news_data_cleaned.csv")
setwd("/Users/sunmingrun/Documents/GitHub/ElectionGPT/NLP")
data <- read_csv("combined_news_data.csv",show_col_types = FALSE)
data_cleaned <-data %>%
select(date,sentiment,relevance,source.uri,source.title) %>%
arrange(date)
#View(data_cleaned)
write_csv(data_cleaned,"news_data_cleaned.csv")
data <- read_csv("combined_news_data.csv",show_col_types = FALSE)
data_cleaned <-data %>%
select(date,sentiment,relevance,source.uri,source.title) %>%
arrange(date)
#View(data_cleaned)
write_csv(data_cleaned,"newsdata_cleaned.csv")
View(subdata2_2)
